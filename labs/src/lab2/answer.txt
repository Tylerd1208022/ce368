andy xu - axl6282
tyler dempski - ted4152

Nsight shows a total thread usage of 225280 (not simulataneous, but across the entire program).

Given our program uses 50 registers per thread and ~12k bytes of shared memory per block, 
we are able to reach full occupancy on each multiprocessor. Since our RTX 2060 gpu has 34 SMs,
34 * 1024 (peak occupancy) = 34816 threads can (and are) scheduled at a time.

Empirically, our method reaches 99.83% occupancy, so the actual value may be slightly lower.


Our solution works out as follows:

First, we load tiles of both M and N into shared memory in fixed 32x32 tile blocks, checking that the values are part
of the original matricies. Threads are then synced.

We then calculate a "limit" to make sure out of bounds accesses do not get computed. The tile elements are then loaded into the
product tile as the sum of the product of the corresponding M and N element. Each thread computes one element of the P tile for the given
block.

Once this computation is completed, the process repeats at loading a new tile into shared memory for M and N, and pTile is incremented
accordingly.

Once the tiles have traversed their respective row and columns of M and N, the pTile is stored into global memory.


Optimization:

All loads from Global Memory into shared memory (represented in our kernel as mTile, nTile, and pTile) are done so in 
row major order. Due to the constant 32 element tile width, all loads involve a load of 32 continuous floats in memory,
giving the ideal 128 byte access on global memory loads and stores.

Due to the use of only row major order operations, our solution yields zero bank conflicts. This can be verified through
profiling the kernel. Notice, we do not need to add padding into shared memory tile allocation, as element access inside a warp is always
done in line with 32-element rows. These 32 simulataneous accesses operate on continuous 4-byte chunks of shared memory, resulting bank usage to result in zero conflicts.