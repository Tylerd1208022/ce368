Lab 4 report
Tyler Dempski - ted4152
Andy Xu - axl6282

Our strategy:

We first began by creating a kernel to handle the reduction/reverse reduction on its own. This would handle only scans of exactly
the same size as the block dimensions. From here, we expanded the kernel to handle data inputs larger than itself by working for multiple
blocks. This, however, causes the blocks to compute separately thus producing incorrect results. For that reason,
we implemented another kernel that would merge the values together, by adding the rightmost value of each preceding private scan
to each element in the private scan handled by each block. This produces correct results.

Our first implementation was very limited in speed due to the memory latency of adding previous values, so we allocated an array
in global memory to store all the rightmost values in all the private histograms. This allowed the second kernel to access
each element in a coalesced manner, as instead of elements sitting BLOCK_SIZE apart in memory, they would be adjacent.

We chose this strategy as it maximizes both occupancy and parallelism, as all elements can be effectively computed in parallel, with
no need to wait for other blocks, aside from the natural synchronization that occurs between kernels.

Speedup: ~0.15x


Optimization 1: Reduce bank conflicts

Here, we add one element of padding every 32 elements to the shared memory in localScanKernel to reduce bank conflicts as the stride length grows.
Before, when stride = 32, we'd have a 32-way bank conflict. With this change, there should be none whatsoever.

Speedup: ~0.2x


Optimization 2: Replace tedious intermediate sum calculation

Previously, our merge kernel would iteratively add previous prefix totals from past blocks to each value in the output.
For instance, to compute the combined a single value in block 5, we'd need add the total sum from block 0, block 1, block 2, 
block 3, and block 4 to its current block-level sum. This leads to repeated work since a value in block 6 would need to repeat
these additions (and add the total sum from block 5).

To resolve this issue, we copy the intermediate total values from each block back to the cpu, compute a prefix sum of these values, then copy 
these combined values back to the gpu. The merge kernel then only needs to add the single combined value to each block-level sum to compute the 
desired prefix sum.

Speedup: ~12.5x


Optimization 3: Nested prefix sum

In the above optimization 2, we merge block sums sequentially. For this optimization, we attempt to re-use the prefix kernel to recursively merge 
intermediate sums and reconstruct the overall prefix sum. This was tricky to implement, and yielded limited speed up for 16777216 elements.
We hypothesize that this will likely give a larger speedup for very large number of elements since this setup better parallelizes
intermediate merge calculations.

Speedup: ~7.5x